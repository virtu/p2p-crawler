# Bitcoin P2P network crawler

Crawler for the Bitcoin P2P network. Collects data greedily. Fully dockerized, simple to
deploy, comes with IPv4, IPv6, Onion and I2P support by default.

## Configuration

Docker containers don't have IPv6 connectivity by default; follow instructions in the
[Enabling IPv6 section](#enabling-ipv6) to enable it.

In addition to storing results to the local file system, the crawler supports persisting
results to Google Cloud Storage. The necessary steps are documented in the [Storing to
GCS section](#storing-to-gcs).

Customizing the crawler is best done via a Docker `.env` file. Instructions are provided
in the [Configuring the crawler section](#configuring-the-crawler).

### Enabling IPv6

Add the following to `/etc/docker/daemon.json`:

```json
{
  "ipv6": true,
  "fixed-cidr-v6": "2001:db8:1::/64"
}
   ```

Enable IPv6 forwarding for the default bridge's prefix (`2001:db8:1::/64`) and the
prefix used by the node crawler (`2001:db8:a::/64`):

```shell
ip6tables -t nat -A POSTROUTING -s 2001:db8:1::/64 ! -o docker0 -j MASQUERADE
ip6tables -t nat -A POSTROUTING -s 2001:db8:a::/64 ! -o docker0 -j MASQUERADE
```

You can check your setup using `docker run --rm -t busybox ping6 www.google.de`.
The `iptables-persistent` package can be used to persist the settings across reboots:
`sudo ip6tables-save -f /etc/iptables/rules.v6` (depending on your Linux distribution,
you may need to change the path).

### Storing to GCS

Create a `secrets/` directory in the project's root directory and put a file
containing your GCS credentials there.

Modify `compose.yaml` as follows:

1. Set the `STORE_TO_GCS` variable for the crawler to `True`.
2. Set the `GCS_BUCKET` and `GCS_LOCATION` variables to appropriate values. The
   `GCS_BUCKET` variable should be set to the name of your GCS bucket; the
   `GCS_LOCATION` to a path inside the bucket. (It can be useful to use host-specific
   `.env` files to set `GCS_LOCATION` in a host-specific way to differentiate between
   results generated by different hosts. See the [Configuring the crawler
   section](#configuring-the-crawler) for details.)
3. Uncomment the `secrets` section in the crawler.
4. Uncomment the global `secrets` section and adjust the path to the file containing the
   secret.

### Configuring the crawler

The crawler can be configured via command-line arguments or environment variables. When
deployed via docker, using environment variables is recommended.

General environment variables should be set in `compose.yaml`.

A host-specific `.env` file can be used to override settings made in `compose.yaml`.
This can be useful, for example, to set the `GCS_LOCATION` on a host-by-host basis.

The following settings are supported:

```markdown
--ip-connect-timeout IP_CONNECT_TIMEOUT
                      Timeout for establishing connections using IP
--ip-message-timeout IP_MESSAGE_TIMEOUT
                      Timeout for replies using IP
--ip-getaddr-timeout IP_GETADDR_TIMEOUT
                      Max. time to receive addr messages from peers using IP
--tor-connect-timeout TOR_CONNECT_TIMEOUT
                      Timeout for establishing connections using TOR
--tor-message-timeout TOR_MESSAGE_TIMEOUT
                      Timeout for replies using TOR
--tor-getaddr-timeout TOR_GETADDR_TIMEOUT
                      Max. time to receive addr messages from peers using TOR
--i2p-connect-timeout I2P_CONNECT_TIMEOUT
                      Timeout for establishing connections using I2P
--i2p-message-timeout I2P_MESSAGE_TIMEOUT
                      Timeout for replies using I2P
--i2p-getaddr-timeout I2P_GETADDR_TIMEOUT
                      Max. time to receive addr messages from peers using I2P
--num-workers NUM_WORKERS
                      Number of crawler coroutines
--node-share NODE_SHARE
                      Share of nodes to query for peers
--delay-start DELAY_START
                      Delay before starting to wait for tor and i2p containers. Default: 10s
--getaddr-retries GETADDR_RETRIES
                      Number of retries for getaddr requests for reachable nodes
--log-level LOG_LEVEL
                      Logging verbosity
--result-path RESULT_PATH
                      Directory for results
--build-info-path BUILD_INFO_PATH
                      Path to build info files
--timestamp TIMESTAMP
                      Timestamp for results
--store-debug-log, --no-store-debug-log
                      Store debug log
--store-to-gcs, --no-store-to-gcs
                      Store results to GCS
--gcs-bucket GCS_BUCKET
                      GCS bucket
--gcs-location GCS_LOCATION
                      GCS location
```

## Running

Once the system has been properly [configured](#configuration), the crawler can be
started by running `make run`.

The `Makefile` will build the crawler image, and download/build all other necessary
images (e.g., the images providing the Onion and I2P proxies used by the crawler to
access these networks).

## To-dos

- [ ] Improve `README.md`: Add a section on what data is getting collected
- [ ] Add support to re-test peers to which a connection could be successfully
      established but which did not complete the handshake or reply to `getaddr`
      messages
- [ ] Make Onion and I2P proxy connections configurable via command line
- [ ] Add deployment helpers (systemd scripts from old repo)
- [ ] Include auxiliary documentation from old repo
